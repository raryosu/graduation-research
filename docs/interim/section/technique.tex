\section{手法}
\subsection{Kullback-Leibler情報量}
偶然を伴う現象は，ある確率分布に従う確率変数の実現値であると考えることができる．
この確率分布を近似するモデル（以後「モデル」）はデータを生成する真の確率分布に
どの程度近いかによって評価することができる．
また，データにモデルを当てはめることは，データから真の確率分布を推定しているものと
みなすことができる．このようにモデルと真の分布が共に確率分布であると見なし，
モデルの評価や推定を行う．

真の分布とモデルの近さを図る客観的な規準としてKullback-Leibler情報量(以後「K-L情報量」）がある．
連続型の確率分布のとき，$g(x)$を真の確率密度関数，$f(x)$をモデルが定める確率密度関数とすると，
モデルに関する真の分布のK-L情報量は$\log\{g(X)/f(X)\}$の期待値を取り\eqref{eq:k-l-div}で表される．
\begin{align}
  \label{eq:k-l-div}
  I(g \mid f) &= E_X\left(\log \left\{\frac{g(X)}{f(X)}\right\}\right)\\\nonumber
              &= \int^{\infty}_{-\infty}\log\left\{\frac{g(x)}{f(x)}\right\}g(x)dx
\end{align}
ただし，$\log$は自然対数で，注記がない限り一貫してこの意味で用いる．

このように，真の分布がわかっている場合にはK-L情報量によってモデルの良し悪しを比較できた．
しかし，通常，真の分布は未知で，真の分布から得られたデータだけが与えられていることがい．
したがって，データからK-L情報量を推定する必要がある．
$g(x)$，$f(x)$をそれぞれ真の分布とモデルに対応する密度関数とすると，
\eqref{eq:k-l-div}より，
\begin{align*}
  I(g \mid f) &= \int^{\infty}_{-\infty}\left\{\log\frac{g(x)}{f(x)}\right\}g(x)dx\\\nonumber
              &= -\int^{\infty}_{-\infty}\{\log f(x)\}g(x)dx - 
                 \left(-\int^{\infty}_{-\infty}\{\log g(x)\}g(x)dx\right)
\end{align*}
となるが，右辺の第2項は定数であり，したがって右辺第1項が大きいほどK-L情報量$I(g \mid f)$は
小さくなることがわかる．右辺第1項の$\int^{\infty}_{-\infty}\{\log f(x)\}g(x)dx$は，
確率密度関数$\log f(x)$の期待値$E( \log f(x))$であり，平均対数尤度と呼ばれている．
ここで，
\begin{align*}
  \sum_{i=1}^{n}\log f(x_i)
\end{align*}
を対数尤度と呼ぶことにすると，$n$個の独立な観測値$\{x_1, x_2, \cdots, x_i\}$が得られると，
この平均対数尤度は，対数尤度の$n$分の1
\begin{align*}
  \frac{1}{n}\sum_{i=1}^{n}\log f(x_i)
\end{align*}
で近似される．
したがって，符号に注意すると，対数尤度が大きいほど，そのモデルは真の分布に近いと考えられる．
このようにして，対数尤度をK-L情報量の推定値を考えることにすると異なったタイプのモデルの
良し悪しも比較できるのである．

ところで，確率変数$(X_1, X_2, \cdots, X_n)$の同時密度関数が$f(x_1, x_2, \cdots, x_n \mid \theta)$で
与えられているものとする．
$\theta$は確率密度関数を規定するパラメータである．この時，観測値$(x_1, x_2, \cdots, x_n)$は
与えられたものとして固定し，$f$を$\theta$の関数と考える時，この関数を\textbf{尤度}と呼び，
$L(\theta)$で表す．すなわち，
\begin{align*}
  L(\theta) = f(x_1, x_2, \cdots, x_n \mid \theta)
\end{align*}
である．特に，確率変数が独立な場合には$(X_1, X_2, \cdots, X_n)$の確率密度関数は，
各$X_i (i = 1, \cdots, n)$の確率密度関数の積に等しいことから，
\begin{align*}
  L(\theta) = f(x_1 \mid \theta)f(x_2 \mid \theta) \cdots f(x_n \mid \theta)
\end{align*}
となる．この両辺の対数をとると，すでに求められた対数尤度関数
\begin{align*}
  l(\theta) = \sum_{i=1}^{n}\log f(x_i \mid \theta)
\end{align*}
が導かれる．

ここでは，平均対数尤度の推定量から対数尤度を直接導入した．
しかし，モデルが確率分布の形で与えられている場合には，まず観測値の同時分布から
尤度を定義し，その対数として対数尤度を求めるほうが都合が良い．
$(X_1, X_2, \cdots, X_n)$が独立でない場合にも，尤度の対数として対数尤度
\begin{align*}
  l(\theta) = \log f(x_1, \cdots, x_n \mid \theta)
\end{align*}
が定義できる．

\subsection{最尤法}
ここまで，データに基づいてK-L情報量の大小を比較するためには対数尤度を比較すれば良いことを示した．
あらかじめ与えられたいくつかのモデルがある場合には，対数尤度が最大となるモデルを選択することによって，
近似的には真の分布にいちばん近いモデルが得られることになる．
したがって，モデルがいくつかの調整できるパラメータを保つ場合には，対数尤度を最大とするように
パラメータの値を選ぶことによって良いモデルが得られることがわかる．
この推定を最大尤度法，略して\textbf{最尤法}と呼ばれている．
また，最尤法で導かれた推定量は\textbf{最尤推定量}と呼ばれ，この最尤推定量によって定められるモデルが
最尤モデルである．
最尤モデルの対数尤度を\textbf{最大対数尤度}という．

\subsection{情報量規準}
情報量規準とは，最尤推定によって当てはめられたモデルが複数個あるときに，その中の一つを選択する規準である．
ここでは，最も有名な情報量規準のひとつであるAIC(Akaike Information Criterion; 赤池情報量規準)$^2)$を例に取り
理論の解説を行う．

\subsection{X-means}
先に紹介したK-meansは，クラスタ数を事前に指定する必要が生じる．
しかし，実際にデータのクラスタリングを行う際，クラスタ数が事前に与えられることは少ない．

X-means$^{3)}$は，データ分布が混合等方Gauss分布から生成されたと想定してクラスタ数推定及び
クラスタリングを行う手法である．
K-meansの逐次繰り返しと，BIC$^{4)}$ (Bayesian Information Criterion; ベイズ情報量規準) による
分割停止規準を用いることで，クラスタ数を推定しクラスタリングを実行する．

具体的には以下の手順で行われる．
\begin{enumerate}
    \item クラスタ数$k$を初期化する (通常は$k=2$) ．
    \item K-meansを実行する．
    \item 次の処理を$j=1$から$j=k$まで繰り返す．
    \begin{enumerate}
        \item クラスタ$j$のBIC$_j$を計算する．
        \item クラスタ$j$に所属するデータに対し，クラスタ数2としてK-meansを行う．
        \item クラスタ数2としてクラスタリングした結果に対しBIC'$_j$を計算する．
        \item BIC$_j$とBIC'$_j$を比較し，BIC'$_j$が大きければクラスタ数$k$に1を足す．
    \end{enumerate}
    \item 前の処理で$k$が増加した場合は処理2へ戻る．そうでない場合は終了する．
\end{enumerate}

X-meansで用いるBICは次のように求められる．
$d$次元のデータ${\bm D}=({\bm x_0}, {\bm x_1}, \cdots, {\bm x_d})$を
$K$個のクラスタに分割することを考える．
モデル$M_j$の評価に用いるBICは以下で与えられる．
\begin{align}
  \label{eq:bic}
  \mathrm{BIC}(M_j) = \hat{l}_j(D) - \frac{p_j}{2}\ln R
\end{align}
$p_j$はモデル$M_j$のパラメータ数であり，$R$は$M_j$のデータ数，
$\hat{l}_j(D)$は$p$変量Gauss分布の対数尤度関数である．

等方Gauss分布を考えると分散$\sigma^2$は\eqref{eq:variance}により表される．
\begin{align}
  \label{eq:variance}
  \hat{\sigma}^2 = \frac{1}{R-K}\sum_i\left({\bm x}_i-{\bm \mu}_{(i)}\right)^2
\end{align}
すると，確率は次で表される．
\begin{align}
  \label{eq:gaussian-distribution}
  \hat{P}(x_i) = \frac{R_{(i)}}{R}\frac{1}{\sqrt{2\pi}\hat{\sigma}^d}
    \exp\left(-\frac{1}{2\hat{\sigma}^2}||{\bm x}_i-{\bm \mu}_{(i)}||^2\right)
\end{align}
ここで${\bm \mu}_{i}$は$d$次元の平均ベクトルである．
したがって対数尤度関数は
\begin{align}
  \label{eq:log-likelihood}
  l(D) &= \log \prod_i P(x_i) \\\nonumber
  &= \sum_i \left( \log\frac{1}{\sqrt{2\pi}\sigma^d}-\frac{1}{2\sigma^2}||{\bm x}_i-{\bm \mu}_{(i)}||^2 + \log\frac{R_{(i)}}{R} \right)
\end{align}
となる．
ここでクラスタ$n (1 < n < K)$のデータ$D_n$に着目する．
クラスタ$n$のデータ数を$R_n$と表記すると，\eqref{eq:log-likelihood}は以下で表される．
\begin{align}
  \begin{split}
    \hat{l}(D_n) &= -\frac{R_n}{2}\log(2\pi) - \frac{R_n \cdot d}{2}\log(\hat{\sigma}^2) -
    \frac{R_n - K}{2}\\ &
    + R_n\log R_n - R_n \log R
  \end{split}
\end{align}
